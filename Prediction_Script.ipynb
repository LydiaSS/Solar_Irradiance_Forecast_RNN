{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction Script.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rYWLcpKXivmb",
        "FznWbafMMPV0",
        "r-2HTwPwQHqu",
        "0fhP4sLbMhZM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5HtnS7bH5vk"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh5h7RrtH3q1"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import math as math\n",
        "import glob as glob\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from plotly.offline import plot, iplot, init_notebook_mode\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW7bi3sAHdFO"
      },
      "source": [
        "# **Data Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr7ZW2xUHrds"
      },
      "source": [
        "## Loading and Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "128K_h2A6d3y"
      },
      "source": [
        "# Link google drive for model and data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyOgrf7JJ2zn"
      },
      "source": [
        "#### Read File In"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data used to simulate predictions will still be the data from weather stations. \n",
        "\n",
        "However ideally, these data would be collected from the hardware devices and transmitted to this server (computer) running the AI model prediction through serial connections.\n",
        "\n",
        "Depending on the format that hardware collected data is, some variables etc. might need to be renamed but the general structure of the script should remain the same."
      ],
      "metadata": {
        "id": "KEczZea2NdCv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGTZ-UFZ11bt"
      },
      "source": [
        "Comment: Assume the csv file is in correct format. The truth is I manually combined the five stations' weather data properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVKUyrAt2nsL"
      },
      "source": [
        "###Read file in with proper date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EQiR_eM2sBV"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/LydiaSS/Solar_Irradiance_Forecast_RNN/main/ACISHourlyData_GoodOrder.csv\", encoding='utf-8', index_col=None, header=0, thousands=',')\n",
        "\n",
        "\n",
        "df['Incoming Solar Rad. (W/m2)'] = pd.to_numeric(df['Incoming Solar Rad. (W/m2)'], downcast=\"float\")\n",
        "\n",
        "def file_cleanup(df):\n",
        "    # Some Cleaning\n",
        "    df.drop_duplicates()\n",
        "    # df = df[df['Incoming Solar Rad. (W/m2)'].notna()]\n",
        "\n",
        "    # Get rid of columns that don't contain any useful data (like comment cols)\n",
        "    preliminary_column_mask = [\n",
        "          'Station Name',\n",
        "          'Date (Local Standard Time)', \n",
        "          'Air Temp. Inst. (C)',\n",
        "          'Air Temp. Min. (C)',\n",
        "          'Air Temp. Max. (C)',\n",
        "          'Air Temp. Avg. (C)',\n",
        "          'Humidity Inst. (%)',\n",
        "          'Humidity Avg. (%)',\n",
        "          'Incoming Solar Rad. (W/m2)',\n",
        "          'Precip. Accumulated (mm)',\n",
        "          'Precip. (mm)',\n",
        "          'Wind Speed 10 m Syno. (km/h)',\n",
        "          'Wind Dir. 10 m Syno. ()',\n",
        "          'Wind Speed 10 m Avg. (km/h)',\n",
        "          'Wind Dir. 10 m Avg. ()',]\n",
        "\n",
        "    df = df[df.columns.intersection(preliminary_column_mask)]\n",
        "\n",
        "    # Rename\n",
        "    df = df.rename(columns={'Wind Dir. 10 m Syno. ()': 'Wind Dir. 10 m Syno. (deg)', \n",
        "                      'Wind Dir. 10 m Avg. ()': 'Wind Dir. 10 m Avg. (deg)'})\n",
        "\n",
        "    df.head()\n",
        "\n",
        "    return df\n",
        "\n",
        "df = file_cleanup(df)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUJhvsOo6Ot_"
      },
      "source": [
        "# Separate the data by station location\n",
        "#grouped = df.groupby(df['Station Name'])\n",
        "\n",
        "df_SOUTH_CAMPUS = df.loc[df['Station Name'].isin([\"Edmonton South Campus UA\"])]\n",
        "df_ST_ALBERT = df.loc[df['Station Name'].isin([\"St. Albert Research\"])]\n",
        "\n",
        "df_OLIVER = df.loc[df['Station Name'].isin([\"Oliver AGDM\"])]\n",
        "df_LEGAL = df.loc[df['Station Name'].isin([\"Legal AGCM\"])]\n",
        "df_ST_FRANCIS = df.loc[df['Station Name'].isin([\"St. Francis AGCM\"])]\n",
        "\n",
        "station_dfs = [df_SOUTH_CAMPUS, df_ST_ALBERT, df_OLIVER, df_LEGAL, df_ST_FRANCIS]\n",
        "# Include weather station geographical location data, note the Lat and Long also have a direction associated but since these\n",
        "#   stations are close to each other the directions are the same and  not considered in this application\n",
        "# Lat and Long data processing: data are taken from the gov of Canada website and converted to decimal using https://www.fcc.gov/media/radio/dms-decimal\n",
        "station_locations = [\n",
        "    {\n",
        "      \"Name\": \"Edmonton South Campus UA\",\n",
        "      \"Latitude\": 53.490014,\n",
        "      \"Longitude\": 113.537778,\n",
        "      \"Elevation (m)\": 670.00,\n",
        "      \"df\": df_SOUTH_CAMPUS,\n",
        "    },\n",
        "    {\n",
        "      \"Name\": \"St. Albert Research\",\n",
        "      \"Latitude\": 53.691944,\n",
        "      \"Longitude\": 113.619722,\n",
        "      \"Elevation (m)\": 687.00,\n",
        "      \"df\": df_ST_ALBERT,\n",
        "    },\n",
        "    {\n",
        "      \"Name\": \"Oliver AGDM\",\n",
        "      \"Latitude\": 53.65,\n",
        "      \"Longitude\": 113.35,\n",
        "      \"Elevation (m)\": 665.00,\n",
        "      \"df\": df_OLIVER,\n",
        "    },\n",
        "    {\n",
        "      \"Name\": \"Legal AGCM\",\n",
        "      \"Latitude\": 54.003056,\n",
        "      \"Longitude\": 113.474445,\n",
        "      \"Elevation (m)\": 680.00,\n",
        "      \"df\": df_LEGAL,\n",
        "    },\n",
        "    {\n",
        "      \"Name\": \"St. Francis AGCM\",\n",
        "      \"Latitude\": 53.294731,\n",
        "      \"Longitude\": 114.320011,\n",
        "      \"Elevation (m)\": 809.00,\n",
        "      \"df\": df_ST_FRANCIS,\n",
        "    },    \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waQdKiLZ8NLF"
      },
      "source": [
        "df_LEGAL[\"date\"] = pd.to_datetime(df['Date (Local Standard Time)'])\n",
        "# I realized that one location doesn't have solar irradiance data before the following date\n",
        "res = df_LEGAL[~(df_LEGAL['date'] < '2018-07-01')]\n",
        "\n",
        "# Let's cut all dataframe to ^ date then\n",
        "for i in range(5):\n",
        "  station_dfs[i][\"date\"] = pd.to_datetime(df['Date (Local Standard Time)'])\n",
        "  station_dfs[i] = station_dfs[i][~(station_dfs[i]['date'] < '2018-07-01')]\n",
        "\n",
        "# Take out timestamps after 10pm and before 6am for all \n",
        "for i in range(5):\n",
        "  # Get all wanted column names from df\n",
        "  station_dfs[i] = station_dfs[i].set_index('date')\n",
        "  station_dfs[i] = station_dfs[i].between_time(\"5:00\", \"22:00\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEFqe7JeYc9N"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXLEAXOZMGZU"
      },
      "source": [
        "# For ease of switching between data files\n",
        "ws_col = 'Wind Speed 10 m Avg. (km/h)'\n",
        "ws_max_col = 'Wind Speed 10 m Syno. (km/h)'\n",
        "wd_col = 'Wind Dir. 10 m Avg. (deg)'\n",
        "date_col = 'Date and Time'\n",
        "temp_col = 'Air Temp. Avg. (C)'\n",
        "solar_irr_col = 'Incoming Solar Rad. (W/m2)'\n",
        "humidity_col = 'Humidity Avg. (%)'\n",
        "\n",
        "# Create a list to hold training only columns (got rid of min and max)\n",
        "train_col_mask = [date_col, temp_col, solar_irr_col]\n",
        "\n",
        "# Save timestamp into its own df first\n",
        "datetime = pd.to_datetime(station_dfs[0].reset_index().pop('Date (Local Standard Time)'), format='%Y-%m-%d %H:%M')\n",
        "timestamp_s = datetime.map(pd.Timestamp.timestamp)\n",
        "\n",
        "# Reset index so index of df starts at 0\n",
        "station_dfs[0] = station_dfs[0].reset_index()\n",
        "\n",
        "for i in range(5):\n",
        "  # Get all wanted column names from df\n",
        "  station_dfs[i] = station_dfs[i][station_dfs[i].columns.intersection(train_col_mask)]\n",
        "  # Equal all the index\n",
        "  station_dfs[i].index = station_dfs[0].index\n",
        "\n",
        "\n",
        "# Combine\n",
        "df_all = pd.concat(station_dfs, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNDfKKVbdshM"
      },
      "source": [
        "### Time Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxrR9yY8eBcn"
      },
      "source": [
        "# For periodicity within a day, we find the total number of seconds in a day\n",
        "day = 24*60*60\n",
        "# For periodicity within a year, we find the total number of seconds in a year\n",
        "year = (365.2425)*day\n",
        "\n",
        "df_all['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "# Apparently cos has worse correlation with the solar irradiance lol\n",
        "df_all['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "df_all['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "df_all['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(np.array(df_all['Day sin'])[:18*2])\n",
        "plt.plot(np.array(df_all['Day cos'])[:18*2])\n",
        "plt.xlabel('Time [h]')\n",
        "plt.title('Time of day signal')\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(np.array(df_all['Year sin'])[:math.floor(year/60**2)])\n",
        "plt.plot(np.array(df_all['Year cos'])[:math.floor(year/60**2)])\n",
        "plt.xlabel('Time [day]')\n",
        "plt.title('Time of year signal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we drop NA after adding time back\n",
        "df_all = df_all.dropna()\n",
        "df_all"
      ],
      "metadata": {
        "id": "YIX3i4ToYEcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7WPOD8djme1"
      },
      "source": [
        "### Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG8mPg4Bjogd"
      },
      "source": [
        "# Normalize the data the same way as in training\n",
        "temp_mean = 3.686254\n",
        "temp_std = 12.321960\n",
        "solar_irr_max = 905.0\n",
        "year_sin_mean = -0.009238\n",
        "year_sin_std = 0.716370\n",
        "year_cos_mean = 0.036401\n",
        "year_cos_std = 0.696765\n",
        "day_sin_mean = -0.114835\n",
        "day_sin_std = 0.750117\n",
        "day_cos_mean = -0.278348\n",
        "day_cos_std = 0.588838\n",
        "\n",
        "# Calculate mean and std\n",
        "mean = [temp_mean, 0, temp_mean, 0, temp_mean, 0, temp_mean, 0, temp_mean, 0, day_sin_mean, day_cos_mean, year_sin_mean, year_cos_mean]\n",
        "std = [temp_std, 1, temp_std, 1, temp_std, 1, temp_std, 1, temp_std, 1, day_sin_std, day_cos_std, year_sin_std, year_cos_std]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize, note solar irradiance is normalized differently\n",
        "solar_irr_normalized = df_all[solar_irr_col]/solar_irr_max\n",
        "\n",
        "df_all_normalized = (df_all - mean) / std\n",
        "df_all_normalized[solar_irr_col] = solar_irr_normalized\n",
        "df_all_normalized.describe().transpose()"
      ],
      "metadata": {
        "id": "3vKKXf89hFP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check with graphs\n",
        "train_std = df_all_normalized.melt(var_name='Column', value_name='Normalized')\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.violinplot(x='Column', y='Normalized', data=train_std)\n",
        "_ = ax.set_xticklabels(df_all_normalized.keys(), rotation=90)"
      ],
      "metadata": {
        "id": "VJ0t8c4yZgTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTUtWJJ4f8d"
      },
      "source": [
        "## Data windowing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk_arF874jlK"
      },
      "source": [
        "Data windowing needs a lot of changes because we have multiple dataframes to generate windows from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0reQg27Bxtw"
      },
      "source": [
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift,\n",
        "               df=df_all_normalized,\n",
        "               label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = df_all_normalized\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(df_all_normalized.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXbDXVw0HbI5"
      },
      "source": [
        "def split_window(self, features):\n",
        "  inputs = features[:, self.input_slice, :]\n",
        "  labels = features[:, self.labels_slice, :]\n",
        "  if self.label_columns is not None:\n",
        "    labels = tf.stack(\n",
        "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "        axis=-1)\n",
        "\n",
        "  # Slicing doesn't preserve static shape information, so set the shapes\n",
        "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "  inputs.set_shape([None, self.input_width, None])\n",
        "  labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "WindowGenerator.split_window = split_window"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FznWbafMMPV0"
      },
      "source": [
        "### Plot the resulting windows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UucQJjAhMX_H"
      },
      "source": [
        "def plot(self, model=None, plot_col=solar_irr_col, max_subplots=3, test=False):\n",
        "  '''\n",
        "  This function plots the window showing inputs vs labels.\n",
        "  This function plots the prediction vs expected if a model is given, however in this case the label_indices\n",
        "    must match the number of elements in predictions. Double check when creating the window. length of prediction = length of input_indices\n",
        "  '''\n",
        "  if test:\n",
        "    inputs, labels = next(iter(self.train))\n",
        "  else:\n",
        "    inputs, labels = self.example\n",
        "\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plot_col_index = self.column_indices[plot_col]\n",
        "  max_n = min(max_subplots, len(inputs))\n",
        "  for n in range(max_n):\n",
        "    plt.subplot(max_n, 1, n+1)\n",
        "    plt.ylabel(f'{plot_col} [normed]')\n",
        "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "             label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "    if self.label_columns:\n",
        "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "    else:\n",
        "      label_col_index = plot_col_index\n",
        "\n",
        "    if label_col_index is None:\n",
        "      continue\n",
        "\n",
        "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "    if model is not None:\n",
        "      predictions = model(inputs)\n",
        "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                  marker='X', edgecolors='k', label='Predictions',\n",
        "                  c='#ff7f0e', s=64)\n",
        "\n",
        "    if n == 0:\n",
        "      plt.legend()\n",
        "\n",
        "  plt.xlabel('Time [h]')\n",
        "\n",
        "WindowGenerator.plot = plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fhP4sLbMhZM"
      },
      "source": [
        "### Convert Windows to Dataset Pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGRickqcMwrb"
      },
      "source": [
        "```make_dataset``` method will take a time series DataFrame and convert it to a ```tf.data.Dataset``` of (input_window, label_window) pairs using the ```preprocessing.timeseries_dataset_from_array``` function. Converting to tf Dataset will allow access to functions like repeat etc. as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDdFmZKMrr9"
      },
      "source": [
        "def make_dataset(self, data, sequence_stride=18):\n",
        "  data = np.array(data, dtype=np.float32)\n",
        "  # This function splits a dataset into a sliding time series window\n",
        "  # https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array\n",
        "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "      data=data,\n",
        "      targets=None,\n",
        "      sequence_length=self.total_window_size,\n",
        "      sequence_stride=sequence_stride,\n",
        "      # The shuffle can be set to False here for the dataset to be in chronological order\n",
        "      shuffle=True,\n",
        "      batch_size=1,)\n",
        "\n",
        "  ds = ds.map(self.split_window)\n",
        "\n",
        "  return ds\n",
        "\n",
        "WindowGenerator.make_dataset = make_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MGbvvzzO5wq"
      },
      "source": [
        "@property\n",
        "def train(self):\n",
        "  return self.make_dataset(self.train_df)\n",
        "\n",
        "@property\n",
        "def example(self):\n",
        "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "  # Lydia: I added the following line of code\n",
        "  #   This ensures that everytime WindowGenerator.example is called, we can get a new iteration\n",
        "  #   of result from the test dataset.\n",
        "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "  result = getattr(self, '_example', None)\n",
        "  if result is None:\n",
        "    # No example batch was found, so get one from the `.train` dataset\n",
        "    result = next(iter(self.train))\n",
        "    # And cache it for next time\n",
        "    self._example = result\n",
        "  return result\n",
        "\n",
        "WindowGenerator.train = train\n",
        "WindowGenerator.example = example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1tt01gqZhXx"
      },
      "source": [
        "# Single Output Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model_name = 'model5'\n",
        "lstm_model = tf.keras.models.load_model(model_name)\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "id": "50kLOQ2cKBaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to Solar Irradiance"
      ],
      "metadata": {
        "id": "J5j-rpSPkABW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain the prediction result of at a particular time.\n",
        "\n",
        "For demonstration purpose let's just try to grab the last frame from the dataset for now."
      ],
      "metadata": {
        "id": "xJ9cbZxEkYmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=0):\n",
        "\tdataX = []\n",
        "\tfor i in range(len(dataset)-look_back):\n",
        "\t\ta = dataset[i:(i+look_back+1)]\n",
        "\t\tdataX.append(a)\n",
        "\treturn np.array(dataX)\n",
        " \n",
        "def reshape_dataset(dataset, samples=1, steps=18, columns=14):\n",
        "  dataX = np.reshape(dataset,(samples, steps, columns))\n",
        "  return dataX\n",
        "\n",
        "def get_data_frame_by_date(date, df_pred, df_actual, window_size):\n",
        "  required_time = pd.to_datetime(date, format='%Y-%m-%d %H:%M')\n",
        "\n",
        "  index = datetime.to_list().index(required_time)\n",
        "\n",
        "  if index <= window_size:\n",
        "    print(\"Does not have enough data before the specified time to make predictions.\")\n",
        "\n",
        "  prediction = df_pred.iloc[index-window_size: index]\n",
        "\n",
        "  theoretical = df_actual.iloc[index-window_size:index+1]\n",
        "\n",
        "  return prediction, theoretical\n",
        "\n",
        "\n",
        "\n",
        "def unormalize_predictions(prediction, mean, std):\n",
        "  # For current normalization method, there should not be data below 0\n",
        "  for i in range(len(prediction[0])):\n",
        "    number = prediction[0][i]\n",
        "    if number < 0:\n",
        "      prediction[0][i] = 0\n",
        "  prediction = prediction*std+mean\n",
        "  return prediction\n"
      ],
      "metadata": {
        "id": "UWHMIfKGIfsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(predictions, samples=1, steps=18, columns=14, plot_col_index = 0, theoretical=None):\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  theoretical_indices = range(1, len(theoretical)+1)\n",
        "  label_indices = range(2, steps+2)\n",
        "  \n",
        "  for n in range(samples):\n",
        "    plt.subplot(samples, 1, n+1)\n",
        "    plt.ylabel(f'Solar Irradiance')\n",
        "\n",
        "    if samples == 1:\n",
        "      plt.plot(theoretical_indices, theoretical.iloc[:, plot_col_index],\n",
        "              label='Inputs', marker='.', zorder=-10)\n",
        "      plt.scatter(theoretical_indices, theoretical.iloc[:, plot_col_index],\n",
        "                  edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "    else:\n",
        "      plt.plot(theoretical_indices, theoretical.iloc[n, :, plot_col_index],\n",
        "              label='Inputs', marker='.', zorder=-10)\n",
        "      plt.scatter(theoretical_indices, theoretical.iloc[n, :, plot_col_index],\n",
        "                  edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "      \n",
        "    # Plot predictions\n",
        "    plt.scatter(label_indices, predictions[n, :],\n",
        "                  marker='X', edgecolors='k', label='Predictions',\n",
        "                  c='#ff7f0e', s=64)\n",
        "\n",
        "    if n == 0:\n",
        "      plt.legend()\n",
        "\n",
        "  plt.xlabel('Time [h]')"
      ],
      "metadata": {
        "id": "QK2avh-cDfMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_plot_predictions(predictions, theoretical, steps=18, columns=14, plot_col = solar_irr_col):\n",
        "  # Create indices\n",
        "  # TODO: make this hour relevant\n",
        "  theoretical_indices = range(1, len(theoretical)+1)\n",
        "  label_indices = range(2, steps+2)\n",
        "\n",
        "  # Create dataframe for predictions to use for px\n",
        "  df_pred = pd.DataFrame(predictions[0, :], columns = [plot_col])\n",
        "  df_pred['index'] = label_indices\n",
        "\n",
        "  # Create dataframe for theoretical data\n",
        "  df_theory = pd.DataFrame(theoretical.iloc[:, 1], columns = [plot_col])\n",
        "  df_theory['index'] = theoretical_indices\n",
        "\n",
        "  # fig = px.line(df_theory, x=\"index\", y=plot_col, markers=True)\n",
        "  # fig.show() \n",
        "  # # Plot predictions\n",
        "  # fig.add_scatter(df_pred, x=\"index\", y=plot_col)\n",
        "\n",
        "  trace1 = go.Scatter(\n",
        "      x=df_theory[\"index\"], y=df_theory[plot_col],\n",
        "      name='Measured Solar Irr',\n",
        "  )\n",
        "  \n",
        "  trace2 = go.Scatter(\n",
        "      x=df_pred[\"index\"], y=df_pred[plot_col],\n",
        "      name='Predicted Solar Irr',\n",
        "  )\n",
        "\n",
        "  fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "  fig.add_trace(trace1)\n",
        "  fig.add_trace(trace2,secondary_y=True)\n",
        "  fig['layout'].update(height = 600, width = 800, title = 'Solar Irradiance Prediction VS Expected' ,xaxis=dict(\n",
        "      tickangle=-90\n",
        "    ))\n",
        "  fig.show()\n",
        "  return fig"
      ],
      "metadata": {
        "id": "gM2KUaP_xRr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data from csv\n",
        "look_back = 0\n",
        "sample = 1\n",
        "step = 18\n",
        "column = 14\n",
        "total_window_size = 18\n",
        "\n",
        "date = \"2020-04-01 22:00\"\n",
        "\n",
        "data, theoretical_data = get_data_frame_by_date(date, df_all_normalized, df_all, total_window_size)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "dataX = reshape_dataset(create_dataset(data, look_back), sample, step, column)\n",
        "\n",
        "# Making Predictions\n",
        "predictions = lstm_model.predict(dataX)\n",
        "\n",
        "true_pred = unormalize_predictions(predictions, 0, solar_irr_max)\n",
        "\n",
        "fig = interactive_plot_predictions(true_pred, theoretical_data)"
      ],
      "metadata": {
        "id": "5J3NjUFjQkpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "Z1pF50vingrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(true_pred[0:,])\n",
        "fig = interactive_plot_predictions(true_pred, theoretical_data)\n",
        "\n",
        "# # Turn on interactive mode\n",
        "# plt.ion()"
      ],
      "metadata": {
        "id": "XAu0KLXud6Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dash"
      ],
      "metadata": {
        "id": "we_tygV5wksQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q dash==1.19.0"
      ],
      "metadata": {
        "id": "T84ANJJew5iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q jupyter_dash==0.3.0"
      ],
      "metadata": {
        "id": "ceoJInVO4R1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q dash-cytoscape"
      ],
      "metadata": {
        "id": "xhPsV5bu4Uc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jupyter_dash import JupyterDash  # pip install dash\n",
        "import dash_cytoscape as cyto  # pip install dash-cytoscape==0.2.0 or higher\n",
        "import dash_html_components as html\n",
        "import dash_core_components as dcc\n",
        "from dash.dependencies import Input, Output, State\n",
        "import pandas as pd  # pip install pandas\n",
        "import plotly.express as px\n",
        "import math\n",
        "from dash import no_update\n",
        "\n",
        "\n",
        "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
        "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)"
      ],
      "metadata": {
        "id": "Z_x86WDpw6kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = {\n",
        "    'background': '#ffffff',\n",
        "    'line': '#ebebeb',\n",
        "    'text': '#000000',\n",
        "}\n",
        "\n",
        "# Get data from csv\n",
        "look_back = 0\n",
        "sample = 1\n",
        "step = 18\n",
        "column = 14\n",
        "total_window_size = 18\n",
        "\n",
        "fig.update_layout(\n",
        "    plot_bgcolor=colors['line'],\n",
        "    paper_bgcolor=colors['background'],\n",
        "    font_color=colors['text']\n",
        ")\n",
        "\n",
        "app.layout = html.Div(style={'backgroundColor': colors['background'], 'margin':'auto', 'width': \"50%\"},\n",
        "    children=[\n",
        "                        \n",
        "      html.H1(children='Multi-location Solar Irradiance Forecast',\n",
        "              style={'textAlign': 'center',\n",
        "                     'color': colors['text']}\n",
        "      ),\n",
        "\n",
        "      html.Div(children='''Group 003''', \n",
        "          style={'textAlign': 'center',\n",
        "                 'color': colors['text']}\n",
        "      ),\n",
        "\n",
        "      html.Div([\n",
        "        \"Date and Hour (yyyy-mm-dd hh:00):  \",\n",
        "        dcc.Input(id='date-input', value='2020-04-01 22:00', type='text'),\n",
        "        # Submit button\n",
        "        html.Button(id='submit-button-state', n_clicks=0, children='Enter'),\n",
        "\n",
        "        dcc.Graph(\n",
        "            id='Forecast',\n",
        "            figure=fig,\n",
        "            style={\"display\": \"block\",\n",
        "                   \n",
        "                   \"margin-left\": \"auto\",\n",
        "                   \"margin-right\": \"auto\",\n",
        "            },\n",
        "        ),\n",
        "      ], style = {'textAlign': 'center', 'justify-content': 'center'}),\n",
        "      html.Br(),\n",
        "      html.Div(id='solar_irr_output'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Plot update call back\n",
        "@app.callback(\n",
        "    Output(component_id='Forecast', component_property='figure'),\n",
        "    # Output(component_id='solar_irr_output', component_property='children'),\n",
        "    Input(component_id='date-input', component_property='value')\n",
        ")\n",
        "\n",
        "# Figure update function\n",
        "def update_figure(datetime):  \n",
        "  data, theoretical_data = get_data_frame_by_date(datetime, df_all_normalized, df_all, total_window_size)\n",
        "\n",
        "  # reshape input to be [samples, time steps, features]\n",
        "  dataX = reshape_dataset(create_dataset(data, look_back), sample, step, column)\n",
        "\n",
        "  # Making Predictions\n",
        "  predictions = lstm_model.predict(dataX)\n",
        "\n",
        "  true_pred = unormalize_predictions(predictions, 0, solar_irr_max)\n",
        "\n",
        "  fig = interactive_plot_predictions(true_pred, theoretical_data)\n",
        "\n",
        "  fig.update_layout(transition_duration=500)\n",
        "\n",
        "  return fig\n",
        "\n",
        "if __name__ =='__main__':\n",
        "    app.run_server(mode=\"external\", debug=True)"
      ],
      "metadata": {
        "id": "3IYS5Gz6w8yF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}